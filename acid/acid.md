# ğŸ“ Dossier ACID â€“ MÃ©thodologie projet DevOps

Ce dossier contient la structure et le dÃ©tail du projet selon la mÃ©thode ACID : **Analyse, Conception, ImplÃ©mentation, DÃ©ploiement**.

---

## ğŸ…°ï¸ A â€“ Analyse

### Objectifs du projet :

- DÃ©velopper une API Spring Boot sÃ©curisÃ©e avec Spring Security
- Centraliser la gestion des secrets avec HashiCorp Vault
- Dockeriser lâ€™application pour faciliter les dÃ©ploiements
- DÃ©ployer lâ€™application sur Kubernetes (Minikube local ou AWS)
- Automatiser le pipeline de CI/CD avec Jenkins
- IntÃ©grer Terraform et Ansible pour la gestion dâ€™infrastructure

### Contraintes techniques :

- Application exposÃ©e sur le port `8081`
- Nom de lâ€™application : `devops-training`
- Lâ€™environnement de dev est basÃ© sur Windows 11 avec Docker Desktop et Minikube

---

## ğŸ…² C â€“ Conception

### Architecture technique :

- **Backend** : Java 17 + Spring Boot 3.4.4
- **Secrets** : HashiCorp Vault (mode dev en local)
- **Conteneurisation** : Docker
- **Orchestration** : Kubernetes via Minikube
- **CI/CD** : Jenkins
- **IaC / automation** : Terraform, Ansible

### Fichiers clÃ©s Ã  concevoir :

- `Dockerfile` : pour builder lâ€™image de lâ€™application
- `deployment.yaml` : dÃ©ploiement Kubernetes
- `service.yaml` : exposition du pod en cluster
- `ingress.yaml` : routage via Ingress Controller
- `Jenkinsfile` : pipeline de build, test et dÃ©ploiement

---

## ğŸ…¸ I â€“ ImplÃ©mentation

### Ã‰tapes rÃ©alisÃ©es :

- CrÃ©ation de lâ€™application Spring Boot avec `/public` et `/private`
- IntÃ©gration de Spring Security avec credentials dynamiques depuis Vault
- Configuration de Vault en mode dev avec secret KV2 (`secret/app`)
- Ã‰criture du `Dockerfile` + build de lâ€™image
- Test de lâ€™application dans Docker

### Ã€ implÃ©menter :

- Fichiers YAML Kubernetes (Deployment, Service, Ingress)
- Pipeline Jenkins avec Ã©tapes : test â†’ build Docker â†’ push image â†’ dÃ©ploiement
- Utilisation de Terraform pour automatiser les manifestes (optionnel)
- Playbook Ansible pour automatiser config locale (Vault, Jenkinsâ€¦)

---

## ğŸ…³ D â€“ DÃ©ploiement

### Environnement cible :

- Minikube (local Kubernetes)
- Vault local (HTTP, non SSL, token root en dev)
- Jenkins local via Docker

### Ã‰tapes de dÃ©ploiement :

ğŸ’¬ Pour consulter les logs dâ€™un pod :

```bash
kubectl logs <nom-du-pod>
```

Exemple :

```bash
kubectl logs devops-training-bcf6f6c97-wgwgg
```

Ou en mode temps rÃ©el :

```bash
kubectl logs -f devops-training-bcf6f6c97-wgwgg
```

1. Builder le jar avec `mvn clean package`
2. Construire lâ€™image Docker : `docker build -t devops-training .`
3. Lancer en local pour test : `docker run -p 8081:8081 devops-training`
4. DÃ©ployer dans Kubernetes : `kubectl apply -f acid/k8s/`
5. Lancer le pipeline Jenkins : clone â†’ test â†’ build â†’ deploy
6. Utiliser Ansible pour provisionner Jenkins et Vault

---

### ğŸ“ Contenu du dossier `acid/k8s/`

#### `Dockerfile`

```dockerfile
FROM openjdk:17-jdk-slim

WORKDIR /app

COPY target/devops-training-0.0.1-SNAPSHOT.jar app.jar

EXPOSE 8081

ENTRYPOINT ["java", "-jar", "app.jar"]
```

ğŸ§  **Explication ligne par ligne :**

- `FROM openjdk:17-jdk-slim` â†’ Utilise une image de base Java 17 lÃ©gÃ¨re
- `WORKDIR /app` â†’ DÃ©finit le rÃ©pertoire de travail Ã  l'intÃ©rieur du conteneur
- `COPY target/... app.jar` â†’ Copie le fichier `.jar` gÃ©nÃ©rÃ© dans le conteneur
- `EXPOSE 8081` â†’ Indique Ã  Docker que lâ€™application Ã©coute sur le port 8081
- `ENTRYPOINT` â†’ Commande exÃ©cutÃ©e automatiquement au dÃ©marrage du conteneur

#### `deployment.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
   name: devops-training
spec:
   replicas: 1
   selector:
      matchLabels:
         app: devops-training
   template:
      metadata:
         labels:
            app: devops-training
      spec:
         containers:
            - name: devops-training
              image: walid/devops-training:latest
              ports:
                 - containerPort: 8081
              env:
                 - name: SPRING_PROFILES_ACTIVE
                   value: "default"
                 - name: VAULT_TOKEN
                   valueFrom:
                      secretKeyRef:
                         name: vault-token-secret
                         key: VAULT_TOKEN
```

ğŸ§  **Explication ligne par ligne :**

- `apiVersion: apps/v1` â†’ Version stable de lâ€™API Kubernetes pour un Deployment
- `kind: Deployment` â†’ Type de ressource : un dÃ©ploiement qui gÃ¨re des pods
- `metadata.name` â†’ Nom du dÃ©ploiement dans le cluster
- `spec.replicas` â†’ Nombre de pods que tu veux faire tourner (1 ici pour du local)
- `selector.matchLabels` et `template.metadata.labels` â†’ Doivent correspondre pour lier le dÃ©ploiement aux pods crÃ©Ã©s
- `spec.template.spec.containers` â†’ Liste des conteneurs Ã  lancer dans le pod
- `image` â†’ Nom de lâ€™image Docker Ã  utiliser
- `containerPort` â†’ Port exposÃ© dans le pod (doit correspondre Ã  celui de ton app)
- `env` â†’ Variables dâ€™environnement (ici, pour activer un profil Spring par exemple)

#### `service.yaml`

```yaml
apiVersion: v1
kind: Service
metadata:
  name: devops-training
spec:
  type: ClusterIP
  selector:
    app: devops-training
  ports:
    - protocol: TCP
      port: 8081
      targetPort: 8081
```

ğŸ§  **Explication ligne par ligne :**

- `apiVersion: v1` â†’ Version de lâ€™API Kubernetes pour les objets de type `Service`
- `kind: Service` â†’ Ce fichier dÃ©finit un service, une ressource rÃ©seau interne Ã  Kubernetes
- `metadata.name` â†’ Le nom du service (utilisÃ© par lâ€™ingress ou dâ€™autres pods)
- `spec.type: ClusterIP` â†’ Le service est accessible uniquement dans le cluster (par dÃ©faut)
- `spec.selector.app` â†’ Ce service cible les pods ayant le label `app=devops-training`
- `ports.port` â†’ Le port exposÃ© par le service (celui que les clients utiliseront dans le cluster)
- `ports.targetPort` â†’ Le port sur lequel les pods Ã©coutent rÃ©ellement

ğŸ§  **Pourquoi on a besoin du Service :**

- Les pods sont Ã©phÃ©mÃ¨res, leur IP peut changer â†’ le Service fournit une IP ou un nom DNS stable
- Il permet la communication entre ressources du cluster
- Il est utilisÃ© comme point de routage par lâ€™Ingress
- Il agit comme un rÃ©partiteur de charge simple entre plusieurs pods (rÃ©plicas)

#### `ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: devops-training-ingress
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - host: devops.local
      http:
        paths:
          - path: /public
            pathType: Prefix
            backend:
              service:
                name: devops-training
                port:
                  number: 8081
```

ğŸ§  **Explication ligne par ligne :**

- `apiVersion: networking.k8s.io/v1` â†’ Version stable de l'API Kubernetes pour les objets Ingress.
- `kind: Ingress` â†’ Cette ressource gÃ¨re le routage HTTP/HTTPS depuis l'extÃ©rieur du cluster vers les services internes.
- `metadata.name` â†’ Nom de l'Ingress, utilisÃ© pour le rÃ©fÃ©rencer dans le cluster.
- `annotations` â†’ MÃ©tadonnÃ©es utilisÃ©es par le contrÃ´leur Ingress (ici NGINX). La rÃ©Ã©criture d'URL `rewrite-target: /` permet Ã  l'app de ne pas avoir Ã  gÃ©rer les sous-chemins.
- `spec.rules` â†’ Liste des rÃ¨gles de routage en fonction des hÃ´tes (domaines).
- `host: devops.local` â†’ Nom de domaine simulÃ© en local, doit Ãªtre ajoutÃ© dans `/etc/hosts` pour pointer vers Minikube.
- `http.paths` â†’ Routes HTTP Ã  rediriger.
- `path: /public` â†’ Lâ€™Ingress nâ€™interceptera que les requÃªtes `/public`
- `pathType: Prefix` â†’ Type de correspondance du chemin (`Prefix` = tout ce qui commence par `/public`).
- `backend.service.name` â†’ Le nom du service Kubernetes vers lequel on redirige les requÃªtes.
- `backend.service.port.number` â†’ Le port utilisÃ© par ce service (doit correspondre Ã  `targetPort` dans le service).

---

### ğŸ§  Pourquoi utilise-t-on un Ingress ?

- Lâ€™Ingress permet dâ€™exposer **plusieurs applications** sur un mÃªme point dâ€™entrÃ©e HTTP (port 80 ou 443).
- Il agit comme un **reverse proxy** dans le cluster, gÃ©nÃ©ralement gÃ©rÃ© par NGINX ou Traefik.
- Il permet dâ€™utiliser des **noms de domaine** pour accÃ©der aux apps (ex: `devops.local`) au lieu de ports techniques.
- Il simplifie lâ€™accÃ¨s aux services : plus besoin de crÃ©er un `NodePort` ou `LoadBalancer` par application.
- En local avec Minikube, on doit activer lâ€™addon Ingress (`minikube addons enable ingress`) et ajouter lâ€™entrÃ©e `devops.local` dans le fichier `/etc/hosts`.

---

### ğŸš€ DÃ©ploiement sur Minikube

### âš ï¸ Limitations connues sur Windows avec Minikube et Ingress

Sur Windows, avec Minikube (driver Docker Desktop), il peut arriver que l'Ingress Controller fonctionne dans le cluster, mais que le tunnel rÃ©seau Windows bloque l'accÃ¨s externe Ã  `devops.local`.

**Pourquoi ?**
- Minikube utilise un rÃ©seau interne Docker.
- Windows limite l'accÃ¨s aux ports infÃ©rieurs Ã  1024 (80/443).
- Firewall Windows ou Docker peut bloquer l'exposition du service.

**Solutions :**
- Utiliser `minikube service devops-training` pour accÃ©der Ã  l'application localement.
- Continuer la formation sans bloquer, en simulant un accÃ¨s NodePort.
- Passer ensuite sur un vrai cluster cloud pour apprendre Ingress proprement (GKE, EKS...).

â¡ï¸ Nous allons dÃ©sormais continuer la formation en dÃ©ployant sur **Google Kubernetes Engine (GKE)** pour reproduire un vrai environnement professionnel.

---

### ğŸ“š Pourquoi `minikube service` permet d'accÃ©der Ã  l'application ?

### ğŸ“š Pourquoi `minikube service` permet d'accÃ©der Ã  l'application ?

Dans Kubernetes, un `Service` de type `ClusterIP` est normalement **inaccessible depuis l'extÃ©rieur** du cluster. Minikube simule un vrai cluster, donc ton service est interne par dÃ©faut.

Quand tu utilises :

```bash
minikube service devops-training
```

ğŸ‘‰ Minikube crÃ©e **un tunnel temporaire** entre ton poste Windows et ton Service Kubernetes. Cela te fournit une URL locale du type :

```
http://127.0.0.1:65186/
```

ğŸ’¡ Cela permet de tester ton application **sans avoir besoin d'Ingress** ou de LoadBalancer compliquÃ© en local.

| Sans tunnel (`minikube service`) | Avec tunnel (`minikube service`) |
|:---------------------------------|:--------------------------------:|
| Pas d'accÃ¨s au Service           | AccÃ¨s local direct temporaire    |
| Besoin d'Ingress ou LoadBalancer | Tunnel automatique               |
| ComplexitÃ© rÃ©seau                | AccÃ¨s simple via 127.0.0.1        |

âš ï¸ En production (ex: AWS, GCP), `minikube service` n'existe pas :
- Il faut exposer l'application via un **Ingress Controller** ou un **Service de type LoadBalancer**.

---

#### ğŸ’¡ Si l'application n'est pas accessible via http://devops.local/public

Lance cette commande dans un terminal **en mode administrateur**, et laisse-la tourner :

```bash
minikube tunnel
```

> Cela permet de faire le pont entre ta machine et le rÃ©seau interne du cluster Kubernetes (ports 80/443 pour l'Ingress).

---

#### 1. DÃ©marrer Minikube

```bash
minikube start
```

#### 2. Activer le contrÃ´leur Ingress

```bash
minikube addons enable ingress
```

#### 3. Ajouter une entrÃ©e dans /etc/hosts

Obtenez l'adresse IP de Minikube avec :

```bash
minikube ip
```

Puis ajoutez cette ligne (en remplaÃ§ant par l'adresse affichÃ©e) :

```txt
192.168.49.2  devops.local
```

> âœ… Remplacez `192.168.49.2` par l'adresse renvoyÃ©e par la commande ci-dessus

#### 4. Appliquer les fichiers Kubernetes

Depuis la racine du projet :

```bash
kubectl apply -f acid/k8s/
```

#### 5. VÃ©rifier le dÃ©ploiement

```bash
kubectl get pods
kubectl get svc
kubectl get ingress
```

#### 6. AccÃ©der Ã  l'application

Dans un navigateur : [http://devops.local](http://devops.local)

---

### ğŸ“¦ CrÃ©ation d'un Secret Kubernetes pour Vault

Pour Ã©viter de stocker le `VAULT_TOKEN` en clair dans `deployment.yaml`, on utilise un Secret Kubernetes.

#### Commande pour crÃ©er le Secret :

```bash
kubectl create secret generic vault-token-secret --from-literal=VAULT_TOKEN=ton-vrai-token-ici
```

- Cela crÃ©e un secret nommÃ© `vault-token-secret`.
- Le `deployment.yaml` lit la variable d'environnement `VAULT_TOKEN` Ã  partir de ce secret.
- SÃ©curitÃ© renforcÃ©e : aucun token sensible n'est stockÃ© dans Git.

---

### ğŸ“¦ DÃ©ploiement de l'image depuis Docker Hub

#### Ã‰tapes Ã  suivre :

1. **CrÃ©er un compte Docker Hub** (si ce nâ€™est pas dÃ©jÃ  fait) : [https://hub.docker.com/](https://hub.docker.com/)
2. **Te connecter depuis le terminal :**
   ```bash
   docker login
   ```
3. **Taguer lâ€™image avec ton nom Docker Hub :** Remplace `monuser` par ton nom dâ€™utilisateur Docker Hub :
   ```bash
   docker tag devops-training monuser/devops-training:latest
   ```
4. **Pousser lâ€™image sur Docker Hub :**
   ```bash
   docker push monuser/devops-training:latest
   ```
5. **Modifier ton **``** pour pointer vers lâ€™image distante :**
   ```yaml
   image: monuser/devops-training:latest
   ```
6. **RedÃ©ployer dans Kubernetes :**
   ```bash
   kubectl apply -f acid/k8s/
   ```

Tu peux maintenant dÃ©ployer depuis n'importe quelle machine connectÃ©e Ã  Docker Hub ğŸ‘

---

### ğŸ“Œ Remarques :

- Le port `8081` est utilisÃ© pour Ã©viter un conflit avec Jenkins (8080)
- `host.docker.internal` est utilisÃ© pour que Docker communique avec Vault local

---

**Fin du document ACID.**

